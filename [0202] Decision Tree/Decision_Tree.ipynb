{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20adfa0b",
   "metadata": {},
   "source": [
    "[ML] Decision Tree\n",
    "\n",
    "Skim through fundamental machine learning concepts and mathematical implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea1e1b",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "<img src = '0.png' width = '500'>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Decision Tree** represents **Decision Rules** into a flowchart-like tree structures in order to **maximize** the categorization of class data.\n",
    "\n",
    "**Root Node** represents the **population** of the data.\n",
    "\n",
    "**Branch** represents an **outcome** of the decision rule,<br>\n",
    "predicts a **categorical output** from the given input data.\n",
    "\n",
    "**Decision node** represents intermediate stage between Root node and the Leaf node.\n",
    "\n",
    "**Leaf node** is the final subset of a decison tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c7e11",
   "metadata": {},
   "source": [
    "## Classification and Regression Tree (CART)\n",
    "\n",
    "**Decision Tree** can be utilized for both **classification** and **regression** task.\n",
    "\n",
    "Decision Tree trains to **minimize** impurity value.\n",
    "\n",
    "> **Impurity** : a measure of **homoegeneity** of the labels\n",
    "\n",
    "**Classification and Regression Tree (CART)** utilizes **Gini Impurity** to judge splits and determine what predictor variable to use.\n",
    "\n",
    "## Gini Impurity\n",
    "\n",
    "**Gini Impurity** represents the probability of **randomly selecting** an **alternate** class data.\n",
    "\n",
    "$Gini(S) = \\displaystyle \\sum_{i=1}^c\\displaystyle \\sum_{i=1}p_ip_{i'} = $\n",
    "$\\displaystyle \\sum_{i=1}^cp_i\\displaystyle \\sum_{i=1}p_{i'} = $\n",
    "$\\displaystyle \\sum_{i=1}^cp_i(1 - p_i)$\n",
    "\n",
    "$= 1 - \\displaystyle \\sum_{i=1}^c{p_i}^2$\n",
    "\n",
    "<img src = '3.png'>\n",
    "\n",
    "$Gini(S) = 1 - p_+^2 - p_-^2$\n",
    "\n",
    "$S$ : Partitioned data set\n",
    "\n",
    "$P+$ : percentage of '+' labeled data\n",
    "\n",
    "$P-$ : percentage of '-' labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf753b",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "**Information_Gain(S, A)** represents the **subtracted value** of the impurity of the parent node from the impurity of the child nodes.\n",
    "\n",
    "The split with the highest Information gain is chosen.\n",
    "\n",
    "$Information Gain(S, A) = Gini(S) - \\displaystyle \\sum\\frac{|S_v|}{|S|}Gini(S_v)$\n",
    "\n",
    "$s.t.$\n",
    "\n",
    "$Gini(S)$ = Impurity of the parent node\n",
    "\n",
    "$\\displaystyle \\sum\\frac{|S_v|}{|S|}Gini(S_v)$ = Impurity of child nodes.\n",
    "\n",
    "<img src = '4.png'>\n",
    "\n",
    "$0.248 \\ge 0.116$\n",
    "\n",
    "$Information Gain(S, 설비)$ $(0.248)$ is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f9364",
   "metadata": {},
   "source": [
    "# Procedures\n",
    "\n",
    "<img src = '5.png'>\n",
    "\n",
    "1. Gini Impurity\n",
    "\n",
    "***A***\n",
    "\n",
    "$I_G(D_p) = 1 - (\\frac{40}{80}^2 + \\frac{40}{80}^2) = 0.5$\n",
    "\n",
    "$I_G(D_{left}) = 1 - \\frac{30}{40}^2 - \\frac{10}{40}^2 = \\frac{3}{8} = 0.375$\n",
    "\n",
    "$I_G(D_{right}) = 1 - \\frac{10}{40}^2 - \\frac{30}{40}^2 = \\frac{3}{8} = 0.375$\n",
    "\n",
    "$IG_A = 0.5 - \\frac{40}{80} * 0.375 - \\frac{40}{80} * 0.375 = 0.125$\n",
    "\n",
    "***B***\n",
    "\n",
    "$I_G(D_{left}) = 1 - \\frac{20}{60}^2 - \\frac{40}{60}^2 = \\frac{4}{9} = 0.444$\n",
    "\n",
    "$I_G(D_{right}) = 1 - \\frac{20}{20}^2 = 0$\n",
    "\n",
    "$IG_B = 0.5 - \\frac{60}{80} * 0.444 - \\frac{20}{80} * 0 = 0.166$\n",
    "\n",
    "$\\therefore A \\le B$\n",
    "\n",
    "**Split B** is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29f471",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "<img src = '6.png'>\n",
    "\n",
    "**Pruning** removes leaf nodes to prevent **overfitting**\n",
    "\n",
    "> **pre-pruning** : The decision tree stops branching if a certain condition is met (max_depth, min_sample_split)<br>\n",
    "> **post-pruning** : converts insignificant subtrees to leaf nodes after the tree is completed.\n",
    "\n",
    "## Cost-Complexity Pruning\n",
    "\n",
    "**Cost-Complexity Pruning (CCP)** splits node into child nodes when the imprity of the model is imrpoved.\n",
    "\n",
    "$CC(T) = Err(T) + \\alpha|T|$\n",
    "\n",
    "$CC(T)$ : cost-complex variable of the decision tree\n",
    "\n",
    "$Err(T)$ : misclassification rate of validation data\n",
    "\n",
    "$|T|$ : # of leaf nodes\n",
    "\n",
    "$\\alpha$ : hyperparameter (lower -> robust)\n",
    "\n",
    "<img src = '7.png'>\n",
    "\n",
    "$0.15 \\ge 0.13 \\le 0.18$<br>\n",
    "$0.55 \\ge 0.43 \\le 0.38$\n",
    "\n",
    "$\\therefore$ **decion tree 2** is the best option. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
