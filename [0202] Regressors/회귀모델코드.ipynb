{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b661174",
   "metadata": {},
   "source": [
    "# 1. DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78679da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 모델 객체 선언\n",
    "reg_tree = DecisionTreeRegressor() # 회귀모델\n",
    "\n",
    "# train 데이터에 대해서 학습진행\n",
    "reg_tree.fit(train_X, train_y)\n",
    "\n",
    "# HyperParameter 후보군; 딕셔너리 형태로 저장\n",
    "parameters={'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], # 분류에서 gini & entropy를 사용한 것과 다르다.\n",
    "            'max_depth':[5,10,15,20,None]}\n",
    "\n",
    "# HyperParameter를 Tuning\n",
    "model_rt=GridSearchCV(estimator=reg_tree, param_grid=parameters,\n",
    "                     scoring='r2', cv=10, refit=True)\n",
    "# cv=10; 교차검증을 위한 값으로, kfold(train/validation을 나누는 한 방법)를 사용할 수도 있다.\n",
    "# refit=True -> 가장 성능이 좋은 '최적의 하이퍼파라미터'을 찾아 선택(튜닝)된 모델객체를 생성한다.\n",
    "\n",
    "# refit=True(default); 최적의 하이퍼파라미터(criterion='friedman_mse', max_depth=5)를 넣어서 모델 객체를 생성한다.\n",
    "# 즉, reg_tree=DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)를 자동으로 수행해준다.\n",
    "\n",
    "# 객체는 이전에 'refit=True'에 의해 생성되었기에, 따로 최적의 하이퍼파라미터로 모델객체 선언없이\n",
    "# 바로 train 데이터에 대한 학습을 진행하면 된다.\n",
    "model_rt.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d059b",
   "metadata": {},
   "source": [
    "# 2. RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995911b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 객체 선언\n",
    "randomforest=RandomForestRegressor() # 회귀모델\n",
    "\n",
    "# train 데이터에 대해서 학습진행\n",
    "randomforest.fit(train_X, train_y)\n",
    "\n",
    "# HyperParameter 후보군\n",
    "parameters={'max_depth':[3,5,10],\n",
    "            'n_estimators':[100,200,300],\n",
    "            'random_state':[1]} # RandomForest모델에서 랜덤추출이라는 과정이 이뤄짐으로 'random_state'를 Hyper-Parameter로 받는다.\n",
    "\n",
    "# HyperParameter를 Tuning\n",
    "model_rfr=GridSearchCV(estimator=randomforest, param_grid=parameters,\n",
    "                 scoring='r2', cv=10, refit=True)\n",
    "\n",
    "model_rfr.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0c89f",
   "metadata": {},
   "source": [
    "# 3. XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor # sklearn 라이브러리에서 가져오는게 아니다.\n",
    "\n",
    "# 객체 선언\n",
    "xgboost=XGBRegressor(objective ='reg:squarederror') # 회귀모델\n",
    "# objective ='reg:squarederror'; XGBRegressor라이브러리 버전관련 문제\n",
    "\n",
    "# train 데이터에 대해서 학습진행\n",
    "xgboost.fit(train_X, train_y)\n",
    "\n",
    "# HyperParameter 후보군\n",
    "parameters={'max_depth':[3,5,10],\n",
    "            'n_estimators':[100,200,300],\n",
    "            'learning_rate':[1e-3,0.01,0.1,1],\n",
    "            'gamma':[0.5,1,2],\n",
    "            'random_state':[1]}\n",
    "# learning_rate의 경우에 model의 개개인적인 특징보다는 학습시에 적용되는 값이다.\n",
    "# 따라서 xgboost=XGBRegressor(learning_rate=0.01) <- 오류 발생\n",
    "\n",
    "# HyperParameter를 Tuning\n",
    "model_xgb=GridSearchCV(estimator=xgboost, param_grid=parameters,\n",
    "                     scoring='r2', cv=10, refit=True)\n",
    "\n",
    "model_xgb.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350ff68",
   "metadata": {},
   "source": [
    "# 4. LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a62c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor # sklearn 라이브러리에서 가져오는게 아니다.\n",
    "\n",
    "# 객체 선언\n",
    "lightgbm=LGBMRegressor() # 회귀모델\n",
    "\n",
    "# train 데이터에 대해서 학습진행\n",
    "lightgbm.fit(train_X, train_y)\n",
    "\n",
    "# HyperParameter 후보군\n",
    "parameters={'max_depth':[3,5,10],\n",
    "            'n_estimators':[100,200,300],\n",
    "            'learning_rate':[1e-3,0.01,0.1,1],\n",
    "            'random_state':[1]}\n",
    "\n",
    "# HyperParameter를 Tuning\n",
    "model_lgbm=GridSearchCV(estimator=lightgbm, param_grid=parameters,\n",
    "                     scoring='r2', cv=10, refit=True)\n",
    "\n",
    "# 객체는 이전에 'refit=True'에 의해 생성되었기에, 바로 train 데이터에 대한 학습을 진행하면 된다.\n",
    "model_lgbm.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baade85",
   "metadata": {},
   "source": [
    "# 5. 모델평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb004677",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RegressionTree model : {:.3f}\".format(mean_squared_error(train_y, model_rt.predict(train_X))))\n",
    "print(\"RandomForest model : {:.3f}\".format(mean_squared_error(train_y, model_rfr.predict(train_X))))\n",
    "print(\"XGBoost model : {:.3f}\".format(mean_squared_error(train_y, model_xgb.predict(train_X))))\n",
    "print(\"LightGBM model : {:.3f}\".format(mean_squared_error(train_y, model_lgbm.predict(train_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R square score for RegressionTree model : {:.3f}\".format(model_rt.best_score_))\n",
    "print(\"R square score for RandomForest model : {:.3f}\".format(model_rfr.best_score_))\n",
    "print(\"R square score for XGBoost model : {:.3f}\".format(model_xgb.best_score_))\n",
    "print(\"R square score for LightGBM model : {:.3f}\".format(model_lgbm.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
