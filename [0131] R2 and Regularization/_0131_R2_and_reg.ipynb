{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a870b5",
   "metadata": {},
   "source": [
    "[ML] $R^2$ and Regularization\n",
    "\n",
    "Skim through fundamental machine learning concepts and mathematical implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02701e14",
   "metadata": {},
   "source": [
    "## Importance of Scaling Data\n",
    "\n",
    "Regression analysis is very sensitive at **scaling** data.<br>\n",
    "So, it is recommended to minimize influence if the input variables' <br>\n",
    "**range and distribution** is different.\n",
    "\n",
    "**Min-Max Scaling** compares differences according to the **rate of change**.\n",
    "\n",
    "**Standard Scaling** compares how much variables have changed throughout its distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07abd4",
   "metadata": {},
   "source": [
    "## Correlation Coefficient $R^2$\n",
    "\n",
    "<img src = '1.png'>\n",
    "\n",
    "**$R^2$** indicates how much a **predicted value** $\\hat{Y}$ explains the **actual value** $Y$ than the **mean value** $\\overline{Y}$.\n",
    "\n",
    "$R^2$ has a range from $0$ to $1$, and the closer it gets to $1$, the powerful the regression model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209fa42",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "**Overfitting** is when a model is not **general** because it is overly fitted to the train data.\n",
    "\n",
    "So it is recommended to use $R_{adj}^2$ , $R_{pred}^2$ to evaluate the efficiency of the model.\n",
    "\n",
    "**Adjusted $R^2$** $R_{adj}^2$  penalizes model by the number of **input variables**.\n",
    "\n",
    "$R_{adj}^2 = 1 - \\frac{SSE(n-1}{SST(n - p - 1)}$\n",
    "\n",
    "**Predicted $R^2$** $R_{pred}^2$  evaluates model from **validation data.**\n",
    "\n",
    "$R_{pred}^2 = 1 - \\frac{\\Sigma_{i=1}^k(y_i - \\hat{y}_i)^2}{\\Sigma_{i=1}^k(y_i - \\overline{y}_i)^2}$\n",
    "\n",
    "If $R^2$ is **much lesser** than $R_{adj}^2$  and $R_{pred}^2$, the model is overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae9d00",
   "metadata": {},
   "source": [
    "## Variation Index Factor (VIF)\n",
    "\n",
    "**Variation Index Factor (VIF)** evaluates **multicollinearity** between variables.<br>\n",
    "If the variables are highly correlated, the correlation coefficients become highly unstable.\n",
    "\n",
    "$VIF_k = \\frac{1}{1 - R_k^2}$\n",
    "\n",
    "If $VIF \\ge 10$, the variables have multicollinearity problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0f391",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Regularization** removes the effect of negative input variables by adding **constraint $f(\\hat{\\beta})$** to regression coefficients.\n",
    "\n",
    "<img src = '2.png'>\n",
    "\n",
    "$min\\Sigma_{i=1}^n(y_1 - \\hat{y}_i)^2 + f(\\hat{\\beta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec032d6d",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "<img src = '3.png'>\n",
    "\n",
    "**Ridge Regression (L2 Regularization)** substitutes the **sum of the squares of regression coefficients** to contraint $f(\\hat{\\beta})$\n",
    "\n",
    "($\\lambda$ = hyperparameter makes more coeffients $0$ if higher)\n",
    "\n",
    "$Minimize$ $ $ $min\\Sigma_{i=1}^n(y_1 - \\hat{y}_i)^2 + \\lambda\\Sigma_{i=1}^p\\hat{\\beta}_j^2$\n",
    "\n",
    "**Ridge** reduces regression coeffients ***close to $0$.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51739ab",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "<img src = '4.png'>\n",
    "\n",
    "**LASSO (L1 Regularization)** substitutes the **sum of the absolute values of regression coefficients** to contraint $f(\\hat{\\beta})$\n",
    "\n",
    "($\\lambda$ = hyperparameter makes more coeffients $0$ if higher)\n",
    "\n",
    "$Minimize$ $ $ $min\\Sigma_{i=1}^n(y_1 - \\hat{y}_i)^2 + \\lambda\\Sigma_{i=1}^p|\\hat{\\beta}_j|$\n",
    "\n",
    "**LASSO** reduces regression coeffients to $0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
